{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob\n",
    "import pyfits\n",
    "import dicom\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import sklearn, sklearn.preprocessing\n",
    "from skimage.restoration import denoise_nl_means\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def importHeartData(calmFile, stressFile, resize):\n",
    "    \"\"\"\n",
    "    Import heart data and extract the pixel array.\n",
    "    Concatenate and return stress file and calm file.\n",
    "    If resize == 1 resize zoom image to ~[34,34,34].\n",
    "    \"\"\"\n",
    "    calmTmp = dicom.read_file(calmFile).pixel_array\n",
    "    stressTmp = dicom.read_file(stressFile).pixel_array\n",
    "    \n",
    "    calmTmp = cropHeart(calmTmp)\n",
    "    stressTmp = cropHeart(stressTmp)\n",
    "\n",
    "    # Pad the 2d slices with zeros so that they are all the same size\n",
    "    zeroArr0 = np.zeros((34,34,34))\n",
    "    zeroArr1 = np.zeros((34,34,34))\n",
    "    \n",
    "    if resize == 1:      \n",
    "        # Resize the input data\n",
    "        calmRatio = 34.0/np.amax(calmTmp.shape)\n",
    "        stressRatio = 34.0/np.amax(stressTmp.shape)\n",
    "\n",
    "        calmTmp = scipy.ndimage.interpolation.zoom(calmTmp, (calmRatio))\n",
    "        stressTmp = scipy.ndimage.interpolation.zoom(stressTmp, (stressRatio))\n",
    "\n",
    "        zeroArr0[:calmTmp.shape[0],:calmTmp.shape[1],:calmTmp.shape[2]] = calmTmp\n",
    "        zeroArr1[:stressTmp.shape[0],:stressTmp.shape[1],:stressTmp.shape[2]] = stressTmp    \n",
    "        \n",
    "        # Normalise and clean\n",
    "        zeroArr0 = normalise(zeroArr0)\n",
    "        zeroArr1 = normalise(zeroArr1)\n",
    "        #zeroArr0 = denoise_nl_means(zeroArr0, h=0.1, multichannel=False)\n",
    "        #zeroArr1 = denoise_nl_means(zeroArr1, h=0.1, multichannel=False)\n",
    "        \n",
    "    else:\n",
    "        zeroArr0[:calmTmp.shape[0],:calmTmp.shape[1],:calmTmp.shape[2]] = calmTmp\n",
    "        zeroArr1[:stressTmp.shape[0],:stressTmp.shape[1],:stressTmp.shape[2]] = stressTmp\n",
    "\n",
    "    catOut = np.moveaxis(np.array([zeroArr0, zeroArr1]), 0, -1)\n",
    "    return catOut\n",
    "\n",
    "def importDir(parentDir):\n",
    "    \"\"\"\n",
    "    Scan though directories in parent directory; look for dirs labelled \n",
    "    STRESS* or REST* in the imediate subsirs and import any dcm files in them.\n",
    "    Return a dataFile of the concatenated stress and calm *.dcm files.\n",
    "    \"\"\"\n",
    "    tmplst = []\n",
    "    for dirs in os.listdir(parentDir):\n",
    "        cwdStress = glob.glob(parentDir+\"/\"+dirs+\"/STRESS*/*.dcm\")\n",
    "        cwdCalm = glob.glob(parentDir+\"/\"+dirs+\"/REST*/*.dcm\")\n",
    "        tmplst.append(importHeartData(cwdCalm[0], cwdStress[0], 1))\n",
    "        \n",
    "    dataFile = np.array(tmplst)\n",
    "    return dataFile\n",
    "\n",
    "def cropHeart(inp):\n",
    "    \"\"\"\n",
    "    Crop the heart so that all the padding is done away with.\n",
    "    Output cropped heart.\n",
    "    \"\"\"\n",
    "    # argwhere will give you the coordinates of every non-zero point\n",
    "    true_points = np.argwhere(inp)\n",
    "    # take the smallest points and use them as the top left of your crop\n",
    "    top_left = true_points.min(axis=0)\n",
    "    # take the largest points and use them as the bottom right of your crop\n",
    "    bottom_right = true_points.max(axis=0)\n",
    "    out = inp[top_left[0]:bottom_right[0]+1,  # plus 1 because slice isn't\n",
    "          top_left[1]:bottom_right[1]+1,   # inclusive\n",
    "          top_left[2]:bottom_right[2]+1]  \n",
    "    return out\n",
    "\n",
    "def normalise(inData):\n",
    "    \"\"\"\n",
    "    Normalise 3D array.\n",
    "    \"\"\"\n",
    "    inDataAbs = np.fabs(inData)\n",
    "    inDataMax = np.amax(inData)\n",
    "    normalisedData = inDataAbs/inDataMax\n",
    "    return normalisedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do data import\n",
    "normDir = \"./data/nlst\"\n",
    "normDat = importDir(normDir)\n",
    "abDir = \"./data/rlst\"\n",
    "abDat = importDir(abDir)\n",
    "inData = np.concatenate([normDat[:abDat.shape[0]], abDat]) # Normal and abnormal data same number of ppts\n",
    "\n",
    "# Do labelling\n",
    "normLab = np.zeros(normDat.shape[0])[:abDat.shape[0]]\n",
    "abLab = np.ones(abDat.shape[0])\n",
    "labels = np.concatenate([normLab, abLab])\n",
    "    \n",
    "# Mutual shuffle\n",
    "shufData, shufLab = sklearn.utils.shuffle(inData, labels, random_state=1)\n",
    "shufData = np.reshape(shufData,(-1,34,34,34,2))\n",
    "shufLabOH = np.eye(2)[labels.astype(int)] # One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 34, 34, 34, 2)\n",
      "(34, 34, 34)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f423c5be810>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfpFCKfhFCvW/3fZI8yuyXxIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f423c5cf0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(shufData.shape)\n",
    "im = denoise_nl_means(shufData[5,...,0], h=0.1, multichannel=False)\n",
    "plt.imshow(im[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m12.22161\u001b[0m\u001b[0m | time: 4.566s\n",
      "\u001b[2K\r",
      "| Adam | epoch: 008 | loss: 12.22161 - acc: 0.4692 -- iter: 10/52\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-2aa06b505a91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshufData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshufLabOH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.pyc\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 818\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# Retrieve loss value from summary string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.reset_default_graph()\n",
    "tflearn.initializations.normal()\n",
    "\n",
    "# Input layer:\n",
    "net = tflearn.layers.core.input_data(shape=[None, 34, 34, 34, 2])\n",
    "\n",
    "# First layer:\n",
    "net = tflearn.layers.conv.conv_3d(net, 8, [5,5,5],  activation=\"leaky_relu\")\n",
    "net = tflearn.layers.conv.max_pool_3d(net, 2, strides=2)\n",
    "\n",
    "# Second layer:\n",
    "net = tflearn.layers.conv.conv_3d(net, 16, [5,5,5], activation=\"leaky_relu\")\n",
    "net = tflearn.layers.conv.max_pool_3d(net, 2, strides=2)\n",
    "\n",
    "# Fully connected layer\n",
    "net = tflearn.layers.core.fully_connected(net, 1024, regularizer=\"L2\", weight_decay=0.001, activation=\"leaky_relu\")\n",
    "net = tflearn.layers.core.fully_connected(net, 1024, regularizer=\"L2\", weight_decay=0.001, activation=\"leaky_relu\")\n",
    "\n",
    "# Dropout layer:\n",
    "net = tflearn.layers.core.dropout(net, keep_prob=0.5)\n",
    "\n",
    "# Output layer:\n",
    "net = tflearn.layers.core.fully_connected(net, 2, activation=\"softmax\")\n",
    "\n",
    "net = tflearn.layers.estimator.regression(net, optimizer='adam', learning_rate=0.01, loss='categorical_crossentropy')\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(shufData, shufLabOH, batch_size=10, n_epoch=50, show_metric=True, validation_set=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
